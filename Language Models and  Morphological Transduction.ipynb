{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL8T2iZUN2Qj"
   },
   "source": [
    "# Language Models & Morphological Transduction\n",
    "In this project studied some traditional appraoches to a few natural language tasks. First, built some n-gram language models on a corpus of Wikipedia articles, and then designed a finite-state transducer for verb conjugation in Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKt-WVr6OtP4"
   },
   "source": [
    "# Part 1: Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cN2Ja8MNP4qS"
   },
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;In this project, we are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
    "\n",
    "* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n",
    "\n",
    "* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) before estimating your models. The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCkrUjKEBrNp"
   },
   "outputs": [],
   "source": [
    "# Constants (feel free to use these in your code, but do not change them)\n",
    "START = \"<s>\"   # Start-of-sentence token\n",
    "END = \"</s>\"    # End-of-sentence-token\n",
    "UNK = \"<UNK>\"   # Unknown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUdZstjH30DL"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "import sys\n",
    "\n",
    "def preprocess(data, vocab=None):\n",
    "    final_data = []\n",
    "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for paragraph in data:\n",
    "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n",
    "        if vocab is not None:\n",
    "            paragraph = [x if x in vocab else UNK for x in paragraph]\n",
    "        if paragraph == [] or paragraph.count('=') >= 2: continue\n",
    "        sen = []\n",
    "        prev_punct, prev_quot = False, False\n",
    "        for word in paragraph:\n",
    "            if prev_quot:\n",
    "                if word[0] not in lowercase:\n",
    "                    final_data.append(sen)\n",
    "                    sen = []\n",
    "                    prev_punct, prev_quot = False, False\n",
    "            if prev_punct:\n",
    "                if word == '\"':\n",
    "                    prev_punct, prev_quot = False, True\n",
    "                else:\n",
    "                    if word[0] not in lowercase:\n",
    "                        final_data.append(sen)\n",
    "                        sen = []\n",
    "                        prev_punct, prev_quot = False, False\n",
    "            if word in {'.', '?', '!'}: prev_punct = True\n",
    "            sen += [word]\n",
    "        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n",
    "        final_data.append(sen)\n",
    "    vocab_was_none = vocab is None\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "    for i in range(len(final_data)):\n",
    "        final_data[i] = [START] + final_data[i] + [END]\n",
    "        if vocab_was_none:\n",
    "            for word in final_data[i]:\n",
    "                vocab.add(word)\n",
    "    return final_data, vocab\n",
    "\n",
    "def getDataset():\n",
    "    dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid'))\n",
    "    train_dataset, vocab = preprocess(dataset[0])\n",
    "    test_dataset, _ = preprocess(dataset[1], vocab)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swPwiHBHDDkT",
    "outputId": "12fac8bb-19d2-4bee-87cd-6a94f7f2ff0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Wiśniowiecki', \"'s\", 'fighting', 'retreat', 'had', 'a', 'major', 'impact', 'on', 'the', 'course', 'of', 'the', 'war', '.', '</s>']\n",
      "['<s>', '<UNK>', 'from', 'reservoirs', 'significantly', 'reduces', 'the', 'river', \"'s\", 'runoff', ',', 'causing', 'an', 'annual', 'loss', 'of', 'over', '3', 'million', 'acre', 'feet', '(', '3', '@.@', '7', 'km3', ')', 'from', 'mainstem', 'reservoirs', 'alone', '.', '</s>']\n",
      "['<s>', 'By', 'the', 'next', 'morning', ',', 'a', 'cold', 'front', 'to', 'the', 'west', 'forced', 'Tanya', 'to', 'accelerate', 'in', 'a', 'more', 'easterly', 'track', '.', '</s>']\n",
      "['<s>', 'It', 'was', 'written', 'by', 'series', 'creator', 'and', 'executive', 'producer', 'Matthew', 'Weiner', 'and', 'writer', '<UNK>', '<UNK>', ',', 'and', 'directed', 'by', 'Scott', '<UNK>', '.', '</s>']\n",
      "['<s>', 'A', 'few', 'style', 'guides', 'allow', 'double', 'sentence', 'spacing', 'for', 'draft', 'work', ',', 'and', 'the', 'Gregg', 'Reference', 'Manual', 'makes', 'room', 'for', 'double', 'and', 'single', 'sentence', 'spacing', 'based', 'on', 'author', 'preferences', '.', '</s>']\n",
      "['<s>', 'The', 'high', 'scalers', 'received', 'considerable', 'media', 'attention', ',', 'with', 'one', 'worker', 'dubbed', 'the', '\"', 'Human', '<UNK>', '\"', 'for', 'swinging', 'co', '@-@', 'workers', '(', 'and', ',', 'at', 'other', 'times', ',', 'cases', 'of', 'dynamite', ')', 'across', 'the', 'canyon', '.', '</s>']\n",
      "['<s>', 'Efforts', 'to', 'pave', 'the', 'road', 'were', 'under', 'way', 'in', '1950', '.', '</s>']\n",
      "['<s>', 'The', 'Dashashwamedh', 'Ghat', 'is', 'the', 'main', 'and', 'probably', 'the', 'oldest', 'ghat', 'of', 'Varanasi', 'located', 'on', 'the', 'Ganges', ',', 'close', 'to', 'the', 'Kashi', 'Vishwanath', 'Temple', '.', '</s>']\n",
      "['<s>', 'Galentine', \"'s\", 'Day', '\"', 'was', 'seen', 'by', 'more', 'viewers', 'than', 'the', 'previous', 'week', \"'s\", 'episode', '\"', 'Sweetums', '\"', ',', 'which', 'drew', '4', '@.@', '87', 'million', 'viewers', 'and', 'was', 'an', 'itself', 'an', 'increase', 'from', 'previous', 'episodes', '.', '\"', '</s>']\n",
      "['<s>', 'The', 'bell', 'is', 'currently', 'held', 'by', 'the', 'Royal', 'Canadian', 'Legion', ',', '<UNK>', ',', 'British', 'Columbia', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for x in random.sample(train_dataset, 10):\n",
    "        print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM6hNHMqTMt2"
   },
   "source": [
    "## The LanguageModel Class\n",
    "\n",
    "Implemented 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. \n",
    "\n",
    "* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model you're implementing.\n",
    "\n",
    "* <b>`generateSentence(self)`</b>: Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in your vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). You may assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to your language model's distribution. Note that the number of words <TT>n</TT> is not fixed; instead, you should stop the sentence as soon as you generate the stop token <TT>&lt;&sol;s&gt;</TT>.\n",
    "\n",
    "* <b>`getSentenceLogProbability(self, sentence)`</b>:  Return the <em> logarithm of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. You should use the natural logarithm $-$ that is, the base-<em>e</em> logarithm. See the note below about performing your calculations in log space.\n",
    "\n",
    "* <b>`getCorpusPerplexity(self, testCorpus)`</b>: You need to compute the perplexity (normalized inverse log probability) of `testCorpus` according to your model. For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin tells you to compute perplexity as follows: \n",
    "\n",
    "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
    "\n",
    "<b>Implementation Hint:</b> In order to avoid underflow, you will likely need to do all of your calculations in log-space. That is, instead of multiplying probabilities, you should add the logarithms of the probabilities and exponentiate the result:\n",
    "\n",
    "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$\n",
    "\n",
    "Using this property should help you in your implementation of `generateSentence(self)` and `getCorpusPerplexity(self, testCorpus)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "uKO6dHNSS45P"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class LanguageModel(object):\n",
    "    def __init__(self, trainCorpus):\n",
    "        '''\n",
    "        Initialize and train the model (i.e. estimate the model's underlying probability\n",
    "        distribution from the training corpus.)\n",
    "        '''\n",
    "\n",
    "        ### DO NOT EDIT ###\n",
    "        return\n",
    "\n",
    "    def generateSentence(self):\n",
    "        '''\n",
    "        Generate a sentence by drawing words according to the model's probability distribution.\n",
    "        Note: Think about how to set the length of the sentence in a principled way.\n",
    "        '''\n",
    "\n",
    "        ### DO NOT EDIT ###\n",
    "        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n",
    "\n",
    "    def getSentenceLogProbability(self, sentence):\n",
    "        '''\n",
    "        Calculate the log probability of the sentence provided. \n",
    "        '''\n",
    "\n",
    "        ### DO NOT EDIT ###\n",
    "        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n",
    "        \n",
    "    def getCorpusPerplexity(self, testCorpus):\n",
    "        '''\n",
    "        Calculate the perplexity of the corpus provided.\n",
    "        '''\n",
    "\n",
    "        ### DO NOT EDIT ###\n",
    "        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n",
    "\n",
    "    def printSentences(self, n):\n",
    "        '''\n",
    "        Prints n sentences generated by your model.\n",
    "        '''\n",
    "\n",
    "        ### DO NOT EDIT ###\n",
    "        for i in range(n):\n",
    "            sent = self.generateSentence()\n",
    "            prob = self.getSentenceLogProbability(sent)\n",
    "            print('Log Probability:', prob , '\\tSentence:',sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf15l6f3etMV"
   },
   "source": [
    "## Unigram Model\n",
    "\n",
    "Here, Implemented each of the 4 functions described above for an <b>unsmoothed unigram</b> model. The probability distribution of a word is given by $\\hat P(w)$.\n",
    "\n",
    "<font color='green'><b>Hints:</b></font>\n",
    "* <font color='green'>You should use a <b>dictionary</b> to map tokens to their unigram counts.</font>\n",
    "* <font color='green'>Since you never want to generate the start-of-sentence token `<s>`, you should <b>not</b> include it in your counts.</font>\n",
    "* <font color='green'>In general, avoid checking for membership in a list (i.e. avoid `x in lst`). Instead, use sets or dictionaries for this purpose $-$ membership checks are much faster on these data structures.</font>\n",
    "* <font color='green'>Do <b>not</b> modify the training or test corpora by using `.append(...)` or `.pop(...)` on them. This will cause unexpected behavior in the autograder tests, which do not expect you to be changing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2uZdMsqeuf2"
   },
   "outputs": [],
   "source": [
    "class UnigramModel(LanguageModel):\n",
    "  def __init__(self, trainCorpus):\n",
    "    self.word_count=0\n",
    "    self.unigram_fre_dic=dict()\n",
    "    for sentence in trainCorpus:\n",
    "      for word in sentence:\n",
    "        if word!=\"<s>\":\n",
    "          self.word_count+=1\n",
    "          self.unigram_fre_dic[word]=self.unigram_fre_dic.get(word,0)+1              \n",
    "                   \n",
    "\n",
    "  def find_index(self,unigram_word_prob,rand_num):\n",
    "    index=0\n",
    "    for i in range(1,len(unigram_word_prob)):\n",
    "      if unigram_word_prob[i-1]<rand_num and rand_num<=unigram_word_prob[i]:\n",
    "        index= i-1 \n",
    "        break   \n",
    "    return index        \n",
    "\n",
    "  def generateSentence(self):\n",
    "      generated_sentence=[\"<s>\"]\n",
    "      unigram_word_prob=[]\n",
    "      prev_prob=0\n",
    "      unigram_word_prob.append(prev_prob)\n",
    "      for word in self.unigram_fre_dic.keys():\n",
    "        prev_prob+=self.get_unigram_probability(word)\n",
    "        unigram_word_prob.append(prev_prob)  \n",
    "      while(True):\n",
    "        random_num=random.random()\n",
    "        word=list(self.unigram_fre_dic.keys())[self.find_index(unigram_word_prob,random_num)]\n",
    "        generated_sentence.append(word)\n",
    "        if word==\"</s>\":\n",
    "          break       \n",
    "      return generated_sentence\n",
    "\n",
    "  def get_unigram_probability(self, word):\n",
    "    return ((self.unigram_fre_dic[word])/(self.word_count))     \n",
    "\n",
    "  def getSentenceLogProbability(self, sentence):\n",
    "    sentence_prob=0\n",
    "    for word in sentence:\n",
    "      if word!=\"<s>\":\n",
    "        sentence_prob+=math.log(self.get_unigram_probability(word))    \n",
    "    return sentence_prob     \n",
    "\n",
    "  def getCorpusPerplexity(self, testCorpus):\n",
    "    prob_logSum_sentence=0\n",
    "    unigram_count=0\n",
    "    for sentence in testCorpus:\n",
    "      unigram_count+=len(sentence)-1\n",
    "      try:\n",
    "        prob_logSum_sentence-=self.getSentenceLogProbability(sentence)\n",
    "      except:\n",
    "        prob_logSum_sentence-=float('-inf')   \n",
    "    return math.exp(prob_logSum_sentence/unigram_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c6zfDsT-GrU"
   },
   "source": [
    "Testing function that uses very simple training & test corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8uRRgLi6IVt",
    "outputId": "4386dc8f-ea33-4b2b-c39b-a317b707a60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST: generateSentence() ---\n",
      "Test generateSentence() passed!\n",
      "\n",
      "--- TEST: getSentenceLogProbability(...) ---\n",
      "Correct log prob.: -19.08542845 \tYour log prob.: -19.08542845 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
      "Correct log prob.: -114.5001481799 \tYour log prob.: -114.5001481799 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
      "Correct log prob.: -108.7963657053 \tYour log prob.: -108.7963657053 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
      "Correct log prob.: -53.6727664115 \tYour log prob.: -53.6727664115 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
      "Correct log prob.: -55.4645258807 \tYour log prob.: -55.4645258807 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
      "Test getSentenceProbability(...) passed!\n",
      "\n",
      "--- TEST: getCorpusPerplexity(...) ---\n",
      "Correct train perp.: 41.3308239726 \tYour train perp.: 41.3308239726 \t PASSED\n",
      "Correct test perp.: 38.0122981569 \tYour test perp.: 38.0122981569 \t PASSED\n",
      "Test getCorpusPerplexity(...) passed!\n"
     ]
    }
   ],
   "source": [
    "def sanityCheck(model_type):\n",
    "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
    "\n",
    "    #\tRead in the test corpus\n",
    "    train_corpus = [\"By the Late Classic , a network of few <unk> ( few <unk> ) linked various parts of the city , running for several kilometres through its urban core .\",\n",
    "    \"Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for .\"]\n",
    "    test_corpus = [\"Classic few parts of the game allowed for few <unk> <unk> incredible city .\", \n",
    "                   \"Few <unk> realize the difficult network , which linked the game to Sonic .\"]\n",
    "    train_corpus, _ = preprocess(train_corpus)\n",
    "    test_corpus, _ = preprocess(test_corpus)\n",
    "    sentence = preprocess([\"Sonic was difficult .\"])[0][0]\n",
    "\n",
    "    # These are the correct answers (don't change them!)\n",
    "    if model_type == \"unigram\":\n",
    "       senprobs = [-19.08542845, -114.5001481799, -108.7963657053, -53.6727664115, -55.4645258807]\n",
    "       trainPerp, testPerp = 41.3308239726, 38.0122981569\n",
    "       model = UnigramModel(train_corpus)\n",
    "    elif model_type == \"smoothed-unigram\":\n",
    "       senprobs = [-19.0405293515, -115.3479413049, -108.9114348746, -54.8190029616, -55.8122547346]\n",
    "       trainPerp, testPerp = 41.9994393615, 39.9531928383\n",
    "       model = SmoothedUnigramModel(train_corpus)\n",
    "    elif model_type == \"bigram\":\n",
    "       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]\n",
    "       trainPerp, testPerp = 1.3861445461, float('inf')\n",
    "       model = BigramModel(train_corpus)\n",
    "    elif model_type == \"smoothed-bigram\":\n",
    "       senprobs = [-16.355820202, -76.0026113319, -74.2346475108, -47.2885760372, -51.2730261907]\n",
    "       trainPerp, testPerp = 12.2307627397, 26.7193157699\n",
    "       model = SmoothedBigramModelAD(train_corpus)       \n",
    "\n",
    "    print(\"--- TEST: generateSentence() ---\")\n",
    "    modelSen = model.generateSentence()\n",
    "    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)\n",
    "    if senTestPassed:\n",
    "        print (\"Test generateSentence() passed!\")\n",
    "    else:\n",
    "        print (\"Test generateSentence() failed; did not return a list of strings...\")\n",
    "\n",
    "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
    "    sentences = [sentence, *train_corpus, *test_corpus]\n",
    "    failed = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sen, correct_prob = sentences[i], senprobs[i]\n",
    "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
    "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
    "        if prob != correct_prob: failed+=1\n",
    "\n",
    "    if not failed:\n",
    "        print (\"Test getSentenceProbability(...) passed!\")\n",
    "    else:\n",
    "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
    "\n",
    "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
    "    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)\n",
    "    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)\n",
    "\n",
    "    print(\"Correct train perp.:\", trainPerp, '\\tYour train perp.:', train_perp, '\\t', 'PASSED' if trainPerp == train_perp else 'FAILED')\n",
    "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
    "    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp\n",
    "    if train_passed and test_passed:\n",
    "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
    "    else:\n",
    "        print(\"Test getCorpusPerplexity(...) failed on\", \"the training corpus and the testing corpus...\" if not train_passed and not test_passed else \"the testing corpus...\" if not test_passed else \"the training corpus...\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    sanityCheck('unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z8h9U63AkiG"
   },
   "source": [
    "Now, training model on the full WikiText corpus, and evaluate it on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1XHIg0xrUIt",
    "outputId": "c03dc07c-b0fd-4387-d606-7eac01b8cd89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from your model ---------\n",
      "Log Probability: -153.05715591834075 \tSentence: ['<s>', 'which', 'December', 'freedom', '@-@', '.', 'gun', 'of', 'Hamilton', '000', 'the', 'shown', 'called', 'conflicting', 'Captain', '.', 'Florida', 'by', 'his', 'pattern', 'to', 'by', 'the', '</s>']\n",
      "Log Probability: -121.76774992684382 \tSentence: ['<s>', 'hour', '(', 'mass', 'other', 'the', 'one', 'agencies', 'video', '?', '<UNK>', 'season', 'Jovian', 'were', 'literary', 'priests', '</s>']\n",
      "Log Probability: -278.4319904554388 \tSentence: ['<s>', '(', 'Staff', '.', 'Pinochet', '3rd', 'to', 'David', 'Achievement', 'was', '<UNK>', 'single', 'her', 'also', '.', 'study', 'decried', 'Wu', 'song', 'Portugal', 'sold', 'older', 'with', 'west', 'tolerance', 'on', '\"', 'heavy', 'Government', '@.@', ',', 'part', ',', 'the', 'intensity', 'Tree', 'praising', '</s>']\n",
      "Log Probability: -83.39118804549047 \tSentence: ['<s>', 'the', 'Wes', 'perform', \"'s\", 'and', 'sex', 'architect', 'of', 'downstream', '5', 'of', '</s>']\n",
      "Log Probability: -130.1246386241084 \tSentence: ['<s>', 'was', 'was', 'is', 'a', 'subordinate', 'during', 'its', 'video', 'a', '.', '.', 'north', 'were', 'metals', 'character', '<UNK>', 'largest', 'time', 'head', '<UNK>', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 1101.9435880266938\n",
      "Testing Set: 912.157438593044\n"
     ]
    }
   ],
   "source": [
    "def runModel(model_type):\n",
    "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
    "    # Read the corpora\n",
    "    if model_type == 'unigram':\n",
    "        model = UnigramModel(train_dataset)\n",
    "    elif model_type == 'bigram':\n",
    "        model = BigramModel(train_dataset)\n",
    "    elif model_type == 'smoothed-unigram':\n",
    "        model = SmoothedUnigramModel(train_dataset)\n",
    "    else:\n",
    "        model = SmoothedBigramModelAD(train_dataset)\n",
    "\n",
    "    print(\"--------- 5 sentences from your model ---------\")\n",
    "    model.printSentences(5)\n",
    "\n",
    "    print (\"\\n--------- Corpus Perplexities ---------\")\n",
    "    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n",
    "    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    runModel('unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bGyA8vOfvRj"
   },
   "source": [
    "## Smoothed Unigram Model\n",
    "\n",
    "Here, Implemented each of the 4 functions described above for a <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n",
    "\n",
    "In order to smooth the model, needed items are: the number of words in the corpus, $N$, and the number of word types, $S$. The distinction between these is meaningful: $N$ indicates the number of word instances, where $S$ refers to the size of our vocabulary. For example, the sentence <em>the cat saw the dog</em> has four word types (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>dog</em>), but five word tokens (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>the</em>, <em>dog</em>). The token <em>the</em> appears twice in the sentence, but they share the same type <em>the</em>.\n",
    "\n",
    "If $c(w)$ is the frequency of $w$ in the training data, you can compute $P_L(w)$ as follows:\n",
    "\n",
    "$$P_L(w)=\\frac{c(w)+1}{N+S}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzX-UZJPfvRn"
   },
   "outputs": [],
   "source": [
    "class SmoothedUnigramModel(UnigramModel):\n",
    "    def __init__(self, trainCorpus):\n",
    "\n",
    "        UnigramModel.__init__(self,trainCorpus)\n",
    "\n",
    "    def get_unigram_probability(self, word):\n",
    "      return ((self.unigram_fre_dic[word]+1)/(self.word_count +len(self.unigram_fre_dic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBy9QaEdCUbc",
    "outputId": "bcdbd412-58dc-43b6-ccfd-91001b2312d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST: generateSentence() ---\n",
      "Test generateSentence() passed!\n",
      "\n",
      "--- TEST: getSentenceLogProbability(...) ---\n",
      "Correct log prob.: -19.0405293515 \tYour log prob.: -19.0405293515 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
      "Correct log prob.: -115.3479413049 \tYour log prob.: -115.3479413049 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
      "Correct log prob.: -108.9114348746 \tYour log prob.: -108.9114348746 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
      "Correct log prob.: -54.8190029616 \tYour log prob.: -54.8190029616 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
      "Correct log prob.: -55.8122547346 \tYour log prob.: -55.8122547346 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
      "Test getSentenceProbability(...) passed!\n",
      "\n",
      "--- TEST: getCorpusPerplexity(...) ---\n",
      "Correct train perp.: 41.9994393615 \tYour train perp.: 41.9994393615 \t PASSED\n",
      "Correct test perp.: 39.9531928383 \tYour test perp.: 39.9531928383 \t PASSED\n",
      "Test getCorpusPerplexity(...) passed!\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    sanityCheck('smoothed-unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6agWmpjdCWOt",
    "outputId": "ca2d05a9-93c9-4e31-fc7b-16a9d38dd4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from your model ---------\n",
      "Log Probability: -15.94305245410116 \tSentence: ['<s>', 'bleed', '</s>']\n",
      "Log Probability: -162.101700803714 \tSentence: ['<s>', 'consists', ',', 'Many', ',', 'series', 'RIAA', 'types', 'Britain', 'profit', 'detailing', 'touchdown', 'a', 'season', 'exact', 'them', 'physically', 'the', 'the', 'Liberal', 'In', '</s>']\n",
      "Log Probability: -184.53249455480972 \tSentence: ['<s>', 'at', 'therefore', '(', 'against', '<UNK>', 'ESA', 'Cape', '.', 'join', 'Quite', 'from', 'Metallurgical', ')', 'however', ',', 'future', 'to', 'One', '.', 'kn', 'a', 'and', 'actions', 'or', 'had', '</s>']\n",
      "Log Probability: -453.23231511310496 \tSentence: ['<s>', 'did', 'took', 'Blackwell', 'inspiration', 'wrote', 'two', 'south', 'regularly', 'This', 'belongings', 'about', 'commented', 'rest', 'of', 'talks', '<UNK>', 'including', \"'\", \"'\", 'for', 'a', 'Christian', 'job', 'king', 'the', 'of', 'their', 'also', 'of', 'widespread', 'method', 'in', 'projects', 'ceasefire', 'General', '\"', 'the', '.', 'phases', 'months', 'for', 'home', 'featured', 'batsmen', 'a', 'was', 'film', 'of', 'with', 'wearing', ',', 'Rachel', 'by', 'of', 'found', 'closure', 'polygyny', ',', 'not', 'were', \"'t\", 'an', '</s>']\n",
      "Log Probability: -1027.8869482561936 \tSentence: ['<s>', 'he', 'position', ',', 'de', 'and', 'plays', 'the', '<UNK>', 'Cy', 'the', 'were', 'full', '<UNK>', 'album', 'with', 'Seventy', 'their', ',', 'from', 'Engineers', 'the', 'Savage', 'evidence', 'system', ',', 'YouTube', ',', 'des', '<UNK>', 'Searle', 'All', 'the', 'baby', 'optimal', 'Navy', 'interact', ',', 'with', 'Kareen', '1965', 'precedence', 'More', 'full', 'with', 'policy', 'ammunition', 'city', 'slightly', 'The', 'for', 'Godsal', 'Corporation', 'forests', 'eventually', 'Hornung', 'pine', 'nature', 'arrange', 'the', 'the', 'Ben', '$', 'Consul', 'all', 'across', '500', 'and', '300', 'DuMont', 'publishers', 'hits', '1980s', 'of', '.', 'a', 'both', 'prolonged', 'his', 'African', '<UNK>', 'himself', 'Blue', 'on', 'threads', 'amyloid', 'echelons', 'from', 'his', 'establish', '(', '<UNK>', '.', 'debut', 'weight', 'Park', '.', ',', 'the', 'also', 'Another', 'X', 'as', 'the', 'suspect', 'the', 'middle', 'observations', 'for', 'what', 'main', 'them', '.', 'in', 'John', ',', 'on', ',', 'Young', '.', 'and', 'for', 'After', 'strange', 'for', 'called', 'Alexander', 'and', 'at', 'was', 'of', 'No', '.', '.', \"'s\", ',', 'October', 'son', ',', 'the', 'public', '.', '4', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 1103.0243317444856\n",
      "Testing Set: 914.472450228316\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    runModel('smoothed-unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGtcWVMGiEGw"
   },
   "source": [
    "## Bigram Model\n",
    "\n",
    "Here, Implemented each of the 4 functions described above for an <b>unsmoothed bigram</b> model. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ojk_q0YiEGx"
   },
   "outputs": [],
   "source": [
    "class BigramModel(LanguageModel):\n",
    "  def __init__(self, trainCorpus):\n",
    "    self.bigram_fre_dict = dict()\n",
    "    self.unique_bigram = set()\n",
    "    self.total_word_count=0\n",
    "    self.unique_word=set()\n",
    "    self.unigram_fre_dic=dict()\n",
    "    for sentence in trainCorpus:\n",
    "        prev_word = \"<s>\"\n",
    "        for i in range(1,len(sentence)):\n",
    "          word=sentence[i]\n",
    "          self.unique_word.add(word)\n",
    "          self.total_word_count+=1\n",
    "          self.unique_word.add(word)\n",
    "          word_pair=(prev_word, word)\n",
    "          self.unique_bigram.add(word_pair)\n",
    "          self.bigram_fre_dict[prev_word]=self.bigram_fre_dict.get(prev_word,{})\n",
    "          self.bigram_fre_dict[prev_word][word]= self.bigram_fre_dict[prev_word].get(word,0)+1\n",
    "          self.unigram_fre_dic[word]=self.unigram_fre_dic.get(word,0)+1\n",
    "          prev_word = word    \n",
    "    self.num_unique_bigrams = len(self.bigram_fre_dict)\n",
    "\n",
    "  def find_index(self, bigram_word_prob, rand_num):\n",
    "      index = 0\n",
    "      for i in range(1, len(bigram_word_prob)):\n",
    "          if bigram_word_prob[i - 1] < rand_num and rand_num <= bigram_word_prob[i]:\n",
    "              index = i - 1\n",
    "              break\n",
    "      return index\n",
    "\n",
    "  def generateSentence(self):\n",
    "      prev_word=\"<s>\"\n",
    "      generated_sentence = [\"<s>\"]\n",
    "      while (True):\n",
    "        \n",
    "        possible_next_words=list(self.bigram_fre_dict[prev_word])\n",
    "        bigram_word_prob = []\n",
    "        prev_prob = 0\n",
    "        bigram_word_prob.append(prev_prob)\n",
    "        for word in possible_next_words:\n",
    "            prev_prob += self.bigram_fre_dict[prev_word][word]/sum(self.bigram_fre_dict[prev_word].values())\n",
    "            bigram_word_prob.append(prev_prob)            \n",
    "        random_num = random.random()\n",
    "        next_word=list(self.bigram_fre_dict[prev_word].keys())[self.find_index(bigram_word_prob, random_num)]\n",
    "        generated_sentence.append(next_word)\n",
    "        prev_word=next_word    \n",
    "        if next_word==\"</s>\":\n",
    "            break              \n",
    "      return generated_sentence\n",
    "  def get_bigram_probability(self, prev_word, next_word):\n",
    "      next_word_dic=self.bigram_fre_dict.get(prev_word,0)\n",
    "      if next_word_dic==0:\n",
    "        numerator=0\n",
    "      else:  \n",
    "        numerator = next_word_dic.get(next_word, 0)\n",
    "        denominator = sum(next_word_dic.values())\n",
    "      return 0 if numerator == 0 or denominator == 0 else numerator / denominator\n",
    "  def getSentenceLogProbability(self, sentence):\n",
    "      sentence_prob = 0\n",
    "      prev_word = \"<s>\"\n",
    "      for i in range(1,len(sentence)):\n",
    "        prob=self.get_bigram_probability(prev_word, sentence[i])  \n",
    "        if prob==0:\n",
    "          sentence_prob +=float('-inf')\n",
    "          break\n",
    "        else:\n",
    "          log_prob=math.log(prob)\n",
    "          sentence_prob += log_prob\n",
    "        prev_word = sentence[i]  \n",
    "      return sentence_prob\n",
    "\n",
    "  def number_of_bigrams(self, sentences):\n",
    "      count = 0\n",
    "      for sentence in sentences:\n",
    "          count += len(sentence) - 1\n",
    "      return count\n",
    "\n",
    "  def getCorpusPerplexity(self, testCorpus):\n",
    "      prob_logSum_sentence = 0\n",
    "      bigram_count = self.number_of_bigrams(testCorpus)\n",
    "      for sentence in testCorpus:\n",
    "          try:\n",
    "              prob_logSum_sentence -= self.getSentenceLogProbability(sentence)\n",
    "          except:\n",
    "              prob_logSum_sentence -= float('-inf')\n",
    "      return math.exp(prob_logSum_sentence / bigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IHQ8dNvAoZn",
    "outputId": "b55fd1fa-475f-4e02-8a22-c7a62473034f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST: generateSentence() ---\n",
      "Test generateSentence() passed!\n",
      "\n",
      "--- TEST: getSentenceLogProbability(...) ---\n",
      "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
      "Correct log prob.: -10.3450917073 \tYour log prob.: -10.3450917073 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
      "Correct log prob.: -9.2464794186 \tYour log prob.: -9.2464794186 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
      "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
      "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
      "Test getSentenceProbability(...) passed!\n",
      "\n",
      "--- TEST: getCorpusPerplexity(...) ---\n",
      "Correct train perp.: 1.3861445461 \tYour train perp.: 1.3861445461 \t PASSED\n",
      "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
      "Test getCorpusPerplexity(...) passed!\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    sanityCheck('bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKop-qYKCZO5",
    "outputId": "39437b50-f9f8-4a90-bbe0-09fc62dbc13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from your model ---------\n",
      "Log Probability: -94.55618869906539 \tSentence: ['<s>', 'It', 'has', 'said', 'she', 'never', 'experienced', 'when', 'Danny', 'Smith', 'published', 'a', 'mount', ',', 'which', 'called', 'The', 'president', 'of', 'Korea', '.', '</s>']\n",
      "Log Probability: -11.955278808204525 \tSentence: ['<s>', 'Bang', '!', '</s>']\n",
      "Log Probability: -75.35601663729683 \tSentence: ['<s>', 'It', 'was', 'to', 'remain', 'on', 'October', '3', '@.@', '80', 'tackles', 'and', 'the', 'United', 'States', 'Army', '(', '19', '@,@', '000', 'supporters', '.', '</s>']\n",
      "Log Probability: -91.66879772779825 \tSentence: ['<s>', 'Nevertheless', ',', 'directed', 'by', 'Chris', '<UNK>', 'her', 'return', 'to', 'this', 'episode', ',', 'the', 'city', 'of', 'Frankie', 'Gavin', 'and', '\"', 'county', '.', '</s>']\n",
      "Log Probability: -13.66556561474093 \tSentence: ['<s>', 'Most', 'Valuable', 'Player', 'Award', '.', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 76.92394608735728\n",
      "Testing Set: inf\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    runModel('bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPBeyKUsfnnW"
   },
   "source": [
    "## Smoothed Bigram Model\n",
    "\n",
    "Here, Implemented each of the 4 functions described above for a <b>bigram</b> model with <b>absolute discounting</b>. The probability distribution of a word is given by $P_{AD}(w’|w)$.\n",
    "\n",
    "In order to smooth the model, computeed a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, you can compute $D$ as: \n",
    "\n",
    "$$D=\\frac{n_1}{n_1+2n_2}$$ \n",
    "\n",
    "For each word $w$, computed the number of bigram types $ww’$ as follows: \n",
    "\n",
    "$$S(w)=|\\{w’\\mid c(ww’)>0\\}|$$ \n",
    "\n",
    "where $c(ww’)$ is the frequency of $ww’$ in the training data. In other words, $S(w)$ is the number of unique words that follow $w$ at least once in the training data.\n",
    "\n",
    "Finally, computing $P_{AD}(w’|w)$ as follows: \n",
    "\n",
    "$$P_{AD}(w’|w)=\\frac{\\max \\big (c(ww’)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w’)\\bigg )$$ \n",
    "\n",
    "where $c(w)$ is the frequency of $w$ in the training data and $P_L(w’)$ is the Laplace-smoothed unigram probability of $w’$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1klb00wtVtS"
   },
   "outputs": [],
   "source": [
    "         \n",
    "class SmoothedBigramModelAD(BigramModel):\n",
    "  def __init__(self, trainCorpus):\n",
    "    BigramModel.__init__(self,trainCorpus)\n",
    "    self.D=0\n",
    "    n1=0\n",
    "    n2=0\n",
    "    for prev_word in self.bigram_fre_dict.keys():\n",
    "      for next_word in self.bigram_fre_dict[prev_word].keys():\n",
    "        if self.bigram_fre_dict[prev_word][next_word]==1:\n",
    "          n1 +=1\n",
    "        if self.bigram_fre_dict[prev_word][next_word]==2:\n",
    "          n2 +=1  \n",
    "    self.D=n1/(n1+2*n2)\n",
    "      #S(w) - number of unique words that follow w\n",
    "      #c(ww') - frequency of bigram ww' in train corpus\n",
    "      #c(w) - frequency of unigram w in train corpus\n",
    "\n",
    "    self.S_W={}\n",
    "    self.C_W={}\n",
    "    for word in self.bigram_fre_dict:\n",
    "      self.S_W[word]=len(self.bigram_fre_dict[word]) \n",
    "      self.C_W[word]=sum(self.bigram_fre_dict[word].values())\n",
    "    \n",
    "  def get_Laplace_smoothed_unigram_probability(self, word):\n",
    "    num_occurance=self.unigram_fre_dic.get(word,0)\n",
    "    return ((num_occurance+1)/(self.total_word_count +len(self.unique_word)))\n",
    " \n",
    "  def getSentenceLogProbability(self, sentence):\n",
    "    sentence_prob = 0\n",
    "    prev_word = \"<s>\"\n",
    "    for i in range(1,len(sentence)):\n",
    "      prob=self.get_bigram_smoothed_probability(prev_word, sentence[i])  \n",
    "      if prob==0:\n",
    "        sentence_prob +=float('-inf')\n",
    "        break\n",
    "      else:\n",
    "        log_prob=math.log(prob)\n",
    "        sentence_prob += log_prob\n",
    "      prev_word = sentence[i]  \n",
    "    return sentence_prob\n",
    "\n",
    "  def get_bigram_smoothed_probability(self,pre_word,next_word):\n",
    "    freq_bigram=0\n",
    "    fre_word=self.C_W.get(pre_word,0)\n",
    "    if fre_word==0:\n",
    "      return 0\n",
    "    else:\n",
    "      freq_bigram=self.bigram_fre_dict[pre_word].get(next_word,0)\n",
    "    laplase_prob=self.get_Laplace_smoothed_unigram_probability(next_word)\n",
    "    prob_word=max((freq_bigram-self.D),0)/self.C_W[pre_word] +((self.D/self.C_W[pre_word])*self.S_W[pre_word]*laplase_prob) \n",
    "    return prob_word\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msIkMy_rBjqF",
    "outputId": "0a0a71d7-521f-4d5a-c44f-002ebbe23b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST: generateSentence() ---\n",
      "Test generateSentence() passed!\n",
      "\n",
      "--- TEST: getSentenceLogProbability(...) ---\n",
      "Correct log prob.: -16.355820202 \tYour log prob.: -16.355820202 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
      "Correct log prob.: -76.0026113319 \tYour log prob.: -76.0026113319 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
      "Correct log prob.: -74.2346475108 \tYour log prob.: -74.2346475108 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
      "Correct log prob.: -47.2885760372 \tYour log prob.: -47.2885760372 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
      "Correct log prob.: -51.2730261907 \tYour log prob.: -51.2730261907 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
      "Test getSentenceProbability(...) passed!\n",
      "\n",
      "--- TEST: getCorpusPerplexity(...) ---\n",
      "Correct train perp.: 12.2307627397 \tYour train perp.: 12.2307627397 \t PASSED\n",
      "Correct test perp.: 26.7193157699 \tYour test perp.: 26.7193157699 \t PASSED\n",
      "Test getCorpusPerplexity(...) passed!\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    sanityCheck('smoothed-bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVKFHOrYBmqB",
    "outputId": "e2de1df4-b8a3-4026-e7d7-f7b9cad2dfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from your model ---------\n",
      "Log Probability: -86.90866606296277 \tSentence: ['<s>', 'In', '1613', ',', 'modeled', 'after', 'the', 'raising', 'its', 'greatest', 'basketball', '@-@', 'a', 'solar', 'parallax', 'and', 'Hindenburg', '.', '</s>']\n",
      "Log Probability: -140.96797193424015 \tSentence: ['<s>', 'As', 'a', 'bill', 'when', '<UNK>', \"'s\", 'first', '@-@', 'line', 'in', 'use', 'of', 'The', 'Crime', 'Thriller', '(', '21', ',', 'and', 'Devonian', 'period', 'of', 'the', '<UNK>', 'introduced', 'an', 'unfinished', 'at', 'the', 'bottom', '.', '</s>']\n",
      "Log Probability: -87.50110208673092 \tSentence: ['<s>', 'The', 'Légion', 'd', '<UNK>', 'and', 'power', 'to', 'game', 'after', 'Mfume', 'into', 'double', 'the', 'Order', 'of', 'the', 'Ganges', 'valley', '.', '</s>']\n",
      "Log Probability: -182.1982262272302 \tSentence: ['<s>', 'At', 'the', 'General', 'to', 'the', 'largest', 'and', 'not', 'checked', 'into', 'the', 'assassination', 'in', 'association', ',', 'elements', 'like', 'recognizing', 'the', 'rise', 'of', 'Big', 'Boss', 'Man', 'Zero', 'series', 'of', '<UNK>', '\"', ';', 'Douglas', 'Jardine', ',', 'and', 'reached', 'Bir', 'Zeit', 'uprooted', '.', '</s>']\n",
      "Log Probability: -213.9548474339977 \tSentence: ['<s>', 'With', 'this', 'time', 'Rich', 'would', 'host', 'cities', ',', 'and', 'neck', 'and', 'number', 'of', 'Somalia', ',', 'Ferdinand', 'VII', '<UNK>', ',', '<UNK>', 'was', 'a', 'best', 'episodes', 'and', 'thirteen', 'years', 'before', 'going', 'on', 'the', 'song', 'to', 'Steffy', ',', 'swells', 'produced', 'by', '<UNK>', '<UNK>', 'strain', 'of', 'the', 'shrine', '.', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 98.5581292053259\n",
      "Testing Set: 272.57201979320354\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    runModel('smoothed-bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3geZQCtgQC_j"
   },
   "source": [
    "# Part 2: Finite State Transducers\n",
    "\n",
    "Here, Implemented a <b>finite state transducer</b> (FST), which transduces the infinitive form of Spanish verbs to the preterite (past tense) form in the 3rd person singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVc5_Z6tQtvP"
   },
   "source": [
    "## Helper functions\n",
    "Here is a description of the most useful methods of this module:\n",
    "\n",
    "\n",
    "* <b>`FST(self, initialStateName)`</b>: Instantiate an FST with an initial (non-accepting) state named `initialStateName`\n",
    "\n",
    "* <b>`addState(self, name, isFinal=False)`</b>: Add a state `name` to the FST; by default, `isFinal=False` and so the state is not an accepting state.\n",
    "\n",
    "* <b>`addTransition(self, inStateName, inString, outString, outStateName)`</b>: Add a transition between state `inStateName` and state `outStateName`, where both of these states already exist in the FST. The FST can traverse this transition after reading `inString`, and outputs `outString` when it does so.\n",
    "\n",
    "* <b>`addSetTransition(self, inStateName, inStringSet, outStateName)`</b>: Add a transition between state `inStateName` and state `outStateName` for each character in `inStringSet`. For each transition, the FST outputs the same character it reads.\n",
    "\n",
    "The cell also contains a method to read the data and a scoring function that you will call in the \"Test your FST\" section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRMsqYzVTBTK"
   },
   "source": [
    "## Build the FST \n",
    " \n",
    "The FST will finish its analysis for a string after having read its last character. If it accepts the string, it will output the transduced form; if it does not accept it, the FST will output FAIL. The FST may be non-deterministic (that is, a state may have multiple transitions for the same input character), but each accepted string should only have one analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UBzmAs9eXrJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "pdtabulate=lambda df:tabulate(df,headers='keys',tablefmt='psql', showindex=False)\n",
    "\n",
    "def readVerbFile(file):\n",
    "    url='https://drive.google.com/u/0/uc?id=18x48rbiNvWoB54wccc635IVkOHzIfS6K&export=download'\n",
    "    df = pd.read_csv(url, header=None)\n",
    "\n",
    "    return df.values.tolist()\n",
    "\n",
    "verbs = readVerbFile('verbsList.csv')\n",
    "\n",
    "def testFST(print_examples = 'all'):\n",
    "    assert print_examples in {'all', 'incorrect', 'none'}, \"print_examples must be 'all', 'incorrect', or 'none'\"\n",
    "    rule_classes = [('0a*', \"0a* Regular -Car verb (stem end in consonant)\", 'hablar      ==>  habló'), ('0b*', \"0b* Regular -Var verb (stem end in vowel)\", 'pasear      ==>  paseó'), ('0c*', \"0c* Regular -er verb\", 'comer       ==>  comió'), ('0d*', \"0d* Regular -ir verb (excluding -guir, -quir)\", 'abrir       ==>  abrió'), ('1a', \"1a  Verbs in -ñer\", 'tañer       ==>  tañó'), ('1b', \"1b  Verbs in -ñir (excluding -eñir)\", 'gañir       ==>  gañó'), ('2a', \"2a  Verbs in -Ver\", 'leer        ==>  leyó'), ('2b', \"2b  Verbs in -Vir\", 'construir   ==>  construyó'), ('2c*', \"2c* Verbs in -guir (excluding -eguir)\", 'distinguir  ==>  distinguió'), ('2d*', \"2d* Verbs in -quir\", 'delinquir   ==>  delinquió'), ('3a', \"3a  Verbs in -eCir\", 'pedir       ==>  pidió'), ('3b', \"3b  Verbs in -eCCir\", 'sentir      ==>  sintió'), ('3c', \"3c  Verbs in -eCCCir\", 'henchir     ==>  hinchió'), ('3d', \"3d  Verbs in -eguir\", 'seguir      ==>  siguió'), ('3e', \"3e  Verbs in -eñir\", 'heñir       ==>  hiñó')]\n",
    "\n",
    "    f = buildFST()\n",
    "    myParses = f.parseInputList([x[0] for x in verbs])\n",
    "    scores, totals, examples = {}, {}, []\n",
    "\n",
    "    for i in range(len(verbs)):\n",
    "        lemma, form, clas = verbs[i]\n",
    "        output = myParses[i]\n",
    "        scores[clas] = scores.get(clas, 0)\n",
    "        totals[clas] = totals.get(clas, 0) + 1\n",
    "        if print_examples == 'all' or print_examples == 'incorrect' and form != output:\n",
    "            examples += [(lemma, form, output, 'CORRECT' if form == output else 'INCORRECT')]\n",
    "        if form == output: scores[clas] += 1\n",
    "    \n",
    "    if print_examples != 'none' and len(examples) > 0:\n",
    "        examples = pd.DataFrame.from_records(examples, columns = ['Input', 'Correct Output', 'Returned Output', 'Result'])\n",
    "        print(pdtabulate(examples))\n",
    "\n",
    "    data= []\n",
    "    \n",
    "    # We use a scoring method that accounts for (1) the fact that the default FST gets many verbs correct\n",
    "    #                                           (2) the class inbalance among the different rules\n",
    "    # p_scores is for verbs that the default FST gets correct, q_scores for verbs it gets wrong\n",
    "    # Each contains a list of accuracies for each category group (for example, all of the examples where rule 2 applies are in one group)\n",
    "    p_scores, q_scores = {'0*': [], '2*': []}, {'1': [], '2': [], '3': []}\n",
    "    for clas, msg, ex in rule_classes:\n",
    "        acc = scores[clas]/totals[clas]\n",
    "        data += [(msg, ex, scores[clas], totals[clas], 100*acc)]\n",
    "        if '0' in clas: p_scores['0*'].append(acc)\n",
    "        elif '1' in clas: q_scores['1'].append(acc)\n",
    "        elif '2' in clas and '*' not in clas: q_scores['2'].append(acc)\n",
    "        elif '2' in clas and '*' in clas: p_scores['2*'].append(acc)\n",
    "        elif '3' in clas: \n",
    "            if clas in {'3a', '3b', '3c'}: q_scores['3'] += [acc, acc, acc] # Weight these higher than -eguir and -eñir\n",
    "            else: q_scores['3'].append(acc)\n",
    "        else: assert False, 'should not get here ' + clas\n",
    "\n",
    "    p_scores = [sum(v) / len(v) for k, v in p_scores.items()] # Get the average for each category group\n",
    "    q_scores = [sum(v) / len(v) for k, v in q_scores.items()]\n",
    "\n",
    "    p_scores = [p_scores[0]] * 3 + [p_scores[1]] # Weight 0* higher than 2*\n",
    "\n",
    "    # Score by averaging the p and q scores, then applying grading formula\n",
    "    p, q = sum(p_scores) / len(p_scores), sum(q_scores) / len(q_scores)\n",
    "    final_score = q/2 * (1+p) # Score will be 0 when p is 100% and q is 0%; score will be 100% when p and q are both 100%\n",
    "\n",
    "    print('\\nScorecard:')\n",
    "    data = pd.DataFrame.from_records(data, columns = ['Category', 'Example', 'Correct', 'Total', 'Accuracy (%)'])\n",
    "    print(pdtabulate(data))\n",
    "\n",
    "    print(\"Overall Score:\", str(final_score*100) + '%')\n",
    "\n",
    "class Transition:\n",
    "    # string_in\n",
    "    # string_out\n",
    "    def __init__(self, inState, inString, outString, outState):\n",
    "        self.state_in = inState\n",
    "        self.string_in = inString\n",
    "        self.string_out = outString\n",
    "        self.state_out = outState\n",
    "\n",
    "    def equals(self, t):\n",
    "        if self.state_in == t.state_in \\\n",
    "        and self.string_in == t.string_in \\\n",
    "        and self.string_out == t.string_out \\\n",
    "        and self.state_out == t.state_out:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "class FSTstate:\n",
    "    # id: an integer ID of the state\n",
    "    # isFinal: is this a final state?\n",
    "    def __init__(self, n, isF, fst):\n",
    "        self.id = n\n",
    "        self.isFinal = isF\n",
    "        self.transitions = dict() # map inStrings to a set of all possible transitions\n",
    "        self.FST = fst\n",
    "\n",
    "    def addTransition(self, inString, outString, outState):\n",
    "        newTransition = Transition(self, inString, outString, outState)\n",
    "        if inString in self.transitions:\n",
    "            for t in self.transitions[inString]:\n",
    "                if t.equals(newTransition):\n",
    "                    return\n",
    "            self.transitions[inString].add(newTransition)\n",
    "        else:\n",
    "            self.transitions[inString] = set([])\n",
    "            self.transitions[inString].add(newTransition)\n",
    "    \n",
    "    def parseInputFromStartState(self, inString):\n",
    "        parseTuple = (\"\", self.id)\n",
    "        parses = []\n",
    "        (accept, stringParses) = self.parseInput(inString)\n",
    "        if accept:\n",
    "            for p in stringParses:\n",
    "                completeParse = [parseTuple]\n",
    "                completeParse.extend(p)\n",
    "                parses.append(completeParse)\n",
    "        return (accept, parses)\n",
    "\n",
    "    def parseInput(self, inString):\n",
    "        parses = []\n",
    "        isAccepted = True\n",
    "        \n",
    "        DEBUG = False\n",
    "        if DEBUG:\n",
    "            print(\"parseInput: state: \", self.id, \" parsing: \" , inString)\n",
    "        \n",
    "        # Case 1: no suffix\n",
    "        if inString == \"\":\n",
    "            epsilonParses = []\n",
    "            epsilonAccepted = False\n",
    "            # try all epsilon transitions\n",
    "            if \"\" in self.transitions:\n",
    "                transSet = self.transitions[\"\"]\n",
    "                for t in transSet:\n",
    "                    outString = t.string_out\n",
    "                    toStateID = t.state_out\n",
    "                    toState = self.FST.allStates[toStateID]\n",
    "                    parseTuple = (outString, toStateID)\n",
    "                    (suffixAccepted, suffixParses) = toState.parseInput(inString)\n",
    "                    if suffixAccepted:\n",
    "                        epsilonAccepted = True\n",
    "                        if suffixParses == []: #accepts.\n",
    "                            parse_s = [parseTuple]\n",
    "                            epsilonParses.append(parse_s)\n",
    "                        else:\n",
    "                            for s in suffixParses:\n",
    "                                parse_s = [parseTuple]\n",
    "                                parse_s.extend(s)\n",
    "                                epsilonParses.append(parse_s)\n",
    "            # if epsilon is accepted, add all its parses\n",
    "            if epsilonAccepted:\n",
    "                parses.extend(epsilonParses)\n",
    "            # if this is a final state, add an empty parse\n",
    "            if self.isFinal or parses != []:\n",
    "                if DEBUG:\n",
    "                    print(\"Accepted in state \", self.id)\n",
    "                return (True, parses)\n",
    "            else:\n",
    "                if DEBUG:\n",
    "                    print(\"Rejected in state \", self.id)\n",
    "                return (False, None)\n",
    "        # case 2: non-empty suffix: there needs to be one suffix that parses!)\n",
    "        hasAcceptedSuffix = False;\n",
    "        for i in range(0,len(inString)+1):\n",
    "            prefix = inString[0:i]\n",
    "            suffix = inString[i:len(inString)]\n",
    "            if DEBUG:\n",
    "                print(\"\\t prefix: \\'\", prefix, \"\\' I=\", i)\n",
    "            if prefix in self.transitions:\n",
    "                if DEBUG:\n",
    "                     print(\"\\t prefix: \", prefix,  \"suffix: \", suffix, \"I=\", i)\n",
    "                transSet = self.transitions[prefix]\n",
    "                for t in transSet:\n",
    "                    outString = t.string_out\n",
    "                    toStateID = t.state_out\n",
    "                    toState = self.FST.allStates[toStateID]\n",
    "                    parseTuple = (outString, toStateID)\n",
    "                    (suffixAccepted, suffixParses) = toState.parseInput(suffix)\n",
    "                    if suffixAccepted:\n",
    "                        hasAcceptedSuffix = True\n",
    "                        if suffixParses == []:\n",
    "                            parse_s = [parseTuple]\n",
    "                            parses.append(parse_s)\n",
    "                            thisPrefixParses = True\n",
    "                        for s in suffixParses:\n",
    "                            parse_s = [parseTuple]\n",
    "                            parse_s.extend(s)\n",
    "                            parses.append(parse_s)\n",
    "        if hasAcceptedSuffix:\n",
    "            return (True, parses)\n",
    "        else:\n",
    "            return (False, None)\n",
    "                            \n",
    "\n",
    "\n",
    "    def printState(self):\n",
    "        if self.isFinal:\n",
    "            FINAL = \"FINAL\"\n",
    "        else: FINAL = \"\"\n",
    "        print(\"State\", self.id, FINAL)\n",
    "        for inString in self.transitions:\n",
    "            transList = self.transitions[inString]\n",
    "            for t in transList:\n",
    "                print(\"\\t\", inString, \":\", t.string_out, \" => \", t.state_out)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "class FST:\n",
    "    def __init__(self, initialStateName=\"q0\"):\n",
    "        self.nStates = 0\n",
    "        self.initState = FSTstate(initialStateName, False, self) \n",
    "        self.allStates = dict()\n",
    "        self.allStates[initialStateName] = self.initState\n",
    "       \n",
    "    def addState(self, name, isFinal=False):\n",
    "        if name in self.allStates:\n",
    "            print(\"ERROR addState: state\", name, \"exists already\")\n",
    "            sys.exit()\n",
    "        elif len(self.allStates) >= 30:\n",
    "            print(\"ERROR addState: you can't have more than 30 states\")\n",
    "            sys.exit()\n",
    "        else:  \n",
    "            newState = FSTstate(name, isFinal, self)\n",
    "            self.allStates[name] = newState\n",
    "\n",
    "    def addTransition(self, inStateName, inString, outString, outStateName):\n",
    "        if (len(inString) > 1):\n",
    "            print(\"ERROR: addTransition: input string \", inString, \" is longer than one character\")\n",
    "            sys.exit()\n",
    "        if inStateName not in self.allStates:\n",
    "            print(\"ERROR: addTransition: state \", inStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "        if outStateName not in self.allStates:\n",
    "            print(\"ERROR: addTransition: state \", outStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "        inState = self.allStates[inStateName]\n",
    "        inState.addTransition(inString, outString, outStateName)\n",
    "\n",
    "    # epsilon:epsilon\n",
    "    def addEpsilonTransition(self, inStateName, outStateName):\n",
    "        if inStateName not in self.allStates:\n",
    "            print(\"ERROR: addEpsilonTransition: state \", inStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "        if outStateName not in self.allStates:\n",
    "            print(\"ERROR: addEpsilonTransition: state \", outStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "        if inStateName == outStateName:\n",
    "            print(\"ERROR: we don't allow epsilon loops\")\n",
    "            sys.exit()\n",
    "        inState = self.allStates[inStateName]\n",
    "        inState.addTransition(\"\", \"\", outStateName)\n",
    "\n",
    "    # map every element in inStringSet to itself\n",
    "    def addSetTransition(self, inStateName, inStringSet, outStateName):\n",
    "         if inStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetTransition: state \", inStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         if outStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetTransition: state \", outStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         for s in inStringSet:\n",
    "            self.addTransition(inStateName, s, s, outStateName)\n",
    "\n",
    "    # map string to itself\n",
    "    def addSelfTransition(self, inStateName, inString, outStateName):\n",
    "         if inStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetTransition: state \", inStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         if outStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetTransition: state \", outStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         self.addTransition(inStateName, inString, inString, outStateName)\n",
    "\n",
    "    # map every element in inStringSet to outString\n",
    "    def addSetToStringTransition(self, inStateName, inStringSet, outString, outStateName):\n",
    "         if inStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetDummyTransition: state \", inStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         if outStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetDummyTransition: state \", outStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         for s in inStringSet:\n",
    "            self.addTransition(inStateName, s, outString, outStateName)\n",
    "    \n",
    "            \n",
    "    # map every element in inStirngSet to outString\n",
    "    def addSetEpsilonTransition(self, inStateName, inStringSet, outStateName):\n",
    "         if inStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetEpsilonTransition: state \", inStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         if outStateName not in self.allStates:\n",
    "            print(\"ERROR: addSetEpsionTransition: state \", outStateName, \" does not exist\")\n",
    "            sys.exit()\n",
    "         for s in inStringSet:\n",
    "            self.addTransition(inStateName, s, \"\", outStateName)\n",
    "            \n",
    "    def parseInput(self, inString):\n",
    "        SHOW_STATES = False#True\n",
    "        inString = inString.rstrip('\\n')\n",
    "        (canParse, allParses)  = self.initState.parseInputFromStartState(inString)\n",
    "        allParsesAsString = \"\"\n",
    "        if canParse:\n",
    "            for parse in allParses:\n",
    "                for tuple in parse:\n",
    "                    outString, outState = tuple\n",
    "                    allParsesAsString += outString\n",
    "                if SHOW_STATES:\n",
    "                    allParsesAsString += \"\\t  States: \"\n",
    "                    i = 0\n",
    "                    for tuple in parse:\n",
    "                        i += 1\n",
    "                        outString, outState = tuple\n",
    "                        allParsesAsString += outState\n",
    "                        if i < len(parse):\n",
    "                            allParsesAsString += \" => \"\n",
    "                    allParsesAsString += \"; \"\n",
    "          \n",
    "            return True, allParsesAsString\n",
    "        else:\n",
    "            return False, \"FAIL\"\n",
    "\n",
    "    def printFST(self):\n",
    "        print(\"Printing FST\", str(self))\n",
    "        for stateID in self.allStates:\n",
    "            state = self.allStates[stateID]\n",
    "            state.printState()\n",
    "\n",
    "    def parseInputList(self, verbList):\n",
    "        #with open(fileName, \"r\") as f:\n",
    "        nParses = 0\n",
    "        totalStrings = 0\n",
    "        res = []\n",
    "        for verb in verbList:#f:\n",
    "            totalStrings += 1\n",
    "            canParse, parse = self.parseInput(verb)\n",
    "            res += [parse]\n",
    "            if canParse:\n",
    "                nParses += 1\n",
    "        fraction = nParses/totalStrings\n",
    "        print(nParses, \"/\", totalStrings, \"=\", str(fraction*100)+'%', \"of examples parsed\") \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_uolFOTfKvA"
   },
   "source": [
    "## Testing the FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g4odS2-LYc_"
   },
   "outputs": [],
   "source": [
    "# Here are some predefined character sets.\n",
    "A2Z = set('abcçdefghijklmnopqrstuvwxyzáéíóúñü')\n",
    "VOWS = set('aeiouáéíóúü')\n",
    "CONS = A2Z-VOWS\n",
    "\n",
    "# TODO: Impelement your solution here.\n",
    "def buildFST():\n",
    "    l=['g','q']\n",
    "    g_q_set=set(l)\n",
    "    # The states (you need to add more)\n",
    "    f = FST('q1') # q1 is the initial (non-accepting) state\n",
    "    f.addState('q_r')\n",
    "    f.addState('q2')\n",
    "    f.addState('q3')\n",
    "    f.addState('q4')\n",
    "    f.addState('q5')\n",
    "    f.addState('q_epsilon')\n",
    "    f.addState('q_EOW', True) # An accepting state\n",
    "    f.addState('q6')\n",
    "    f.addState('q7')\n",
    "    f.addState('q8')\n",
    "    f.addState('q9')\n",
    "    f.addState('q10')\n",
    "    f.addState('q11')\n",
    "    f.addState('q12')\n",
    "\n",
    "    # The transitions (you need to add more)\n",
    "    f.addSetTransition('q1', A2Z-{'e'}, 'q1') # Self-loop on all letters to transition through stem\n",
    "    #f.addSetTransition('q1', A2Z-VOWS-{'ñ','g','q'}, 'q2')\n",
    "    f.addSetTransition('q1', A2Z-VOWS-{'ñ'}, 'q2')\n",
    "\n",
    "    f.addTransition('q2', 'a', 'ó', 'q_r') # Transition on vowel of ending, with a ==> ó and e ==> ió and i ==> ió\n",
    "    f.addTransition('q2', 'e', 'ió', 'q_r')\n",
    "    f.addTransition('q2', 'i', 'ió', 'q_r')\n",
    "\n",
    "    #Stems ending in ñ: \n",
    "    f.addTransition('q1', 'ñ', 'ñ', 'q3')\n",
    "\n",
    "    #If the stem ends in ñ, the ending is -ó rather than -ió\n",
    "    f.addTransition('q3', 'e', 'ó', 'q_r')\n",
    "    f.addTransition('q3', 'i', 'ó', 'q_r')\n",
    "    f.addTransition('q3', 'a', 'ó', 'q_r')\n",
    "\n",
    "    #f.addTransition('q8', 'ñ', 'ñ', 'q3')\n",
    "    f.addTransition('q8', 'ñ', 'ñ', 'q11')\n",
    "    f.addTransition('q11', 'i', 'ó', 'q_r')\n",
    "    f.addTransition('q8', 'g', 'g', 'q6')\n",
    "    \n",
    "    #Transition from q10 to q8\n",
    "    f.addTransition('q10', 'e', 'i', 'q8')\n",
    "    #Transition from q10 to q9\n",
    "    f.addTransition('q10', 'e', 'e', 'q9')\n",
    "\n",
    "    #Stems ending in a vowel: \n",
    "    f.addSetTransition('q1', A2Z-{'g','q'}, 'q4')\n",
    "    f.addSetTransition('q4', VOWS,'q5')\n",
    "    #If the stem ends in a vowel, the ending is -yó rather than -ió\n",
    "    f.addTransition('q5', 'i', 'yó', 'q_r')\n",
    "    f.addTransition('q5', 'e', 'yó', 'q_r')\n",
    "    f.addTransition('q5', 'a', 'ó', 'q_r')\n",
    "\n",
    "    #does not apply to verbs ending in -guir or -quir\n",
    "    f.addSetTransition('q1',g_q_set,'q6')  \n",
    "    f.addSelfTransition('q6','u','q7')\n",
    "    f.addTransition('q7', 'i', 'ió', 'q_r')\n",
    "    f.addTransition('q7', 'e', 'ió', 'q_r')\n",
    "    f.addTransition('q7', 'a', 'ó', 'q_r')\n",
    "\n",
    "    #For -ir verbs only, if the stem ends in an e followed by any number of consonants, then the e changes to an i\n",
    "    f.addTransition('q1', 'e', 'i', 'q8')\n",
    "    f.addSetTransition('q8', CONS-{'ñ'}, 'q8')\n",
    "    f.addTransition('q8', 'i', 'ió', 'q_r')\n",
    "\n",
    "    f.addTransition('q1', 'e', 'e', 'q9')\n",
    "    f.addSetTransition('q9', CONS, 'q10')\n",
    "    #f.addSetTransition('q9', CONS-{'g'}, 'q10')\n",
    "    f.addSetTransition('q10', CONS, 'q10')\n",
    "\n",
    "    # changing q9 state\n",
    "    #f.addSetTransition('q9', VOWS, 'q9')\n",
    "\n",
    "    f.addTransition('q10', 'e', 'ió', 'q_r')\n",
    "    f.addTransition('q10', 'a', 'ó', 'q_r')\n",
    "\n",
    "    f.addSetTransition('q10', VOWS-{'e'}, 'q1')\n",
    "    # transition q10 to q5 \n",
    "    # trying to change from q10 to q7\n",
    "    #f.addSetTransition('q10', VOWS, 'q5')\n",
    "    f.addSetTransition('q10', VOWS, 'q12')\n",
    "    f.addTransition('q12', 'a', 'ó', 'q_r')\n",
    "    f.addTransition('q_r', 'r', '', 'q_epsilon') # Transition on final r in infinitive - replace with empty string\n",
    "    f.addTransition('q_epsilon', '', '', 'q_EOW') # If see empty string, at end of word\n",
    "    #applying changes after analysis\n",
    "    f.addTransition('q12', 'e', 'yó', 'q_r')\n",
    "    f.addSetTransition('q10', VOWS, 'q4')\n",
    "    f.addSetTransition('q9', VOWS, 'q1')\n",
    "    f.addTransition('q6', 'i', 'i', 'q7')\n",
    "    f.addSetTransition('q9', VOWS, 'q5')\n",
    "    # Return your completed FST\n",
    "    return f\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89ikxuGefPPf",
    "outputId": "ee2457aa-610b-4395-f013-7a80839b096b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3295 / 3295 = 100.0% of examples parsed\n",
      "+-------------+------------------+----------------------+-----------+\n",
      "| Input       | Correct Output   | Returned Output      | Result    |\n",
      "|-------------+------------------+----------------------+-----------|\n",
      "| menguar     | menguó           | menguóminguó         | INCORRECT |\n",
      "| atreguar    | atreguó          | atreguóatriguó       | INCORRECT |\n",
      "| arpegiar    | arpegió          | arpegióarpigió       | INCORRECT |\n",
      "| privilegiar | privilegió       | privilegiópriviligió | INCORRECT |\n",
      "+-------------+------------------+----------------------+-----------+\n",
      "\n",
      "Scorecard:\n",
      "+-----------------------------------------------+-----------------------------+-----------+---------+----------------+\n",
      "| Category                                      | Example                     |   Correct |   Total |   Accuracy (%) |\n",
      "|-----------------------------------------------+-----------------------------+-----------+---------+----------------|\n",
      "| 0a* Regular -Car verb (stem end in consonant) | hablar      ==>  habló      |      2371 |    2371 |       100      |\n",
      "| 0b* Regular -Var verb (stem end in vowel)     | pasear      ==>  paseó      |       555 |     559 |        99.2844 |\n",
      "| 0c* Regular -er verb                          | comer       ==>  comió      |       150 |     150 |       100      |\n",
      "| 0d* Regular -ir verb (excluding -guir, -quir) | abrir       ==>  abrió      |       130 |     130 |       100      |\n",
      "| 1a  Verbs in -ñer                             | tañer       ==>  tañó       |         2 |       2 |       100      |\n",
      "| 1b  Verbs in -ñir (excluding -eñir)           | gañir       ==>  gañó       |         6 |       6 |       100      |\n",
      "| 2a  Verbs in -Ver                             | leer        ==>  leyó       |         8 |       8 |       100      |\n",
      "| 2b  Verbs in -Vir                             | construir   ==>  construyó  |        20 |      20 |       100      |\n",
      "| 2c* Verbs in -guir (excluding -eguir)         | distinguir  ==>  distinguió |         2 |       2 |       100      |\n",
      "| 2d* Verbs in -quir                            | delinquir   ==>  delinquió  |         3 |       3 |       100      |\n",
      "| 3a  Verbs in -eCir                            | pedir       ==>  pidió      |        20 |      20 |       100      |\n",
      "| 3b  Verbs in -eCCir                           | sentir      ==>  sintió     |        13 |      13 |       100      |\n",
      "| 3c  Verbs in -eCCCir                          | henchir     ==>  hinchió    |         2 |       2 |       100      |\n",
      "| 3d  Verbs in -eguir                           | seguir      ==>  siguió     |         4 |       4 |       100      |\n",
      "| 3e  Verbs in -eñir                            | heñir       ==>  hiñó       |         5 |       5 |       100      |\n",
      "+-----------------------------------------------+-----------------------------+-----------+---------+----------------+\n",
      "Overall Score: 99.93291592128801%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    testFST(print_examples='incorrect')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AL8T2iZUN2Qj",
    "DtMMWXC0Emwq",
    "jywbgcdQXwfz"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
